---
layout: post  
title: "HashSet의 중복 제거 원리와 O(n), O(log n) 성능 차이"
categories: [computer-science]
tags: [HashSet, Big-O, Time-Complexity, Java]
---

자료구조와 알고리즘을 공부하다 보면
왜 어떤 연산은 데이터가 많아져도 빠르고,
어떤 연산은 데이터의 크기에 따라 급격히 느려지는지 궁금해진다.

예를 들어 `HashSet`은 내부적으로 어떻게 동작하길래
중복 여부를 매우 빠르게 판단할 수 있을까?
그리고 이러한 성능 차이는
시간 복잡도 관점에서 `O(n)`과 `O(log n)`과 같은 개념으로
어떻게 설명할 수 있을까?

이 글에서는
`O(n)`과 `O(log n)`의 성능 차이를 예시와 함께 살펴보고,
데이터가 1백만 개일 때 각각 어느 정도의 연산이 필요한지 비교한 뒤,
이를 바탕으로 `HashSet`이 해시 기반 구조를 통해
왜 효율적인 중복 체크가 가능한지까지 함께 정리해본다.

---

## 1. 시간 복잡도(Time-Complexity)

시간 복잡도(Time Complexity)는
알고리즘이 입력 데이터의 크기에 따라
**얼마나 많은 연산을 수행하는지**를 나타내는 개념이다.

여기서 중요한 점은
“실제 걸리는 시간”이 아니라
**데이터의 크기가 증가할 때 연산 횟수가 어떻게 변하는가**에 초점을 둔다는 것이다.

이러한 관점에서
시간 복잡도는 `Big-O` 표기법으로 표현되며,
알고리즘의 성능을 상대적으로 비교하는 기준으로 사용된다.

즉, `Big-O`는  
- 하드웨어 성능
- 실행 환경
- 언어 차이  

같은 요소들을 배제하고,
**알고리즘 자체의 성장 패턴**만을 설명하기 위한 도구라고 볼 수 있다.

---

## 2. O(n) 과 O(log n)

### 2-1) O(n)

`O(n)`은
데이터의 크기만큼 연산 횟수가 증가하는 경우이다.

즉, 데이터가 2배가 되면
필요한 연산 횟수도 거의 2배가 된다.

이는 n배의 데이터가 있다면
필요 연산 횟수가 거의 n배가 된다는 것이다.

실생활 예시로 보면,
사서가 정렬되지 않은 책장에서 특정 책을 찾기 위해
처음부터 끝까지 하나씩 확인하는 과정과 비슷하다.

운이 좋으면 빨리 찾을 수도 있지만,
운이 없는 경우 모든 책을 확인해야 한다.

### 2-2) O(log n)

`O(log n)`은
연산을 수행할 때마다
문제의 크기를 절반씩 줄여나가는 경우를 의미한다.

대표적인 예는 사전에서 단어를 찾는 방식이다.

가운데를 펼쳐보고
앞부분인지, 뒷부분인지 판단한 뒤
나머지 절반을 버리는 과정을 반복한다.

이 방식은
데이터의 크기가 커져도
필요한 단계 수가 매우 천천히 증가한다.

---

#### 예시) 데이터가 1,000,000개일 때

`O(n)`의 경우 데이터 연산을
평균적으로 약 500,000번,
최악의 경우 1,000,000번을 연산한다.

`O(log n)`의 경우
log₂(1,000,000) ≈ 20,
약 20번의 연산으로 데이터를 처리한다.

즉, 데이터의 크기가 커질수록
`O(n)`과 `O(log n)`의 성능 차이는  
비교할 수 없을 정도로 벌어지게 된다.

---

## 3. 시간 복잡도에서 바라본 HashSet

이제 시간 복잡도의 개념을 바탕으로
`HashSet`의 성능을 살펴볼 차례다.

`HashSet`은
중복을 허용하지 않는 컬렉션으로,
값의 존재 여부를 매우 빠르게 판단할 수 있다.

겉보기에는 단순히 **중복을 제거해주는 컬렉션**처럼 보이지만,
내부에서는 **해시(Hash)**를 기반으로 하기에
효율적인 중복 체크를 수행한다.

### 3-1) HashSet의 내부 구조

실제 `HashSet`은 독립적 자료구조가 아닌
`HashMap`을 기반으로 구현되어 있다.

`HashSet`에 저장되는 값은
내부적으로 `HashMap`의 key로 저장되며,
value에는 항상 동일한 더미 객체가 사용된다.

즉, `HashSet.add(value)`는
내부적으로 `HashMap.put(value, PRESENT)`
를 호출하는 것과 같다.

### 3-2) 중복 제거 메커니즘

`HashSet`이 중복을 판단하는 기준은
`hashCode()`와 `equals()` 메서드의 조합이다.

중복 판단 과정은 다음과 같은 순서로 이루어진다.

1. 객체의 `hashCode()`를 호출해 해시값을 계산한다
2. 해시값을 기반으로 저장할 버킷(bucket)을 찾는다
3. 해당 버킷에 이미 객체가 존재하는지 확인한다
4. 존재할 경우 `equals()`를 호출해 실제로 같은 객체인지 비교한다
5. 같다고 판단되면 중복으로 간주하여 저장하지 않는다

### 3-3) hashCode와 equals

해시값이 같다고 해서  
두 객체가 반드시 같은 객체는 아니다.  
이를 **해시 충돌(hash collision)**이라고 한다.

따라서 `HashSet`은  
먼저 `hashCode()`를 사용해 후보 범위를 좁히고,  
그다음 `equals()`를 사용해  
실제 동등한 객체인지 확인하는  
2단계 검증 방식을 사용한다.

### 3-4) HashSet의 성능 관점

`HashSet`이 효율적인 중복 체크를 할 수 있는 이유는
선형 탐색 `O(n)`이 아닌
해시 기반 접근 방식을 사용하기 때문이다.

리스트 기반 중복 체크는
처음부터 끝까지 비교해야 하지만,
`HashSet`은 해시값을 통해
필요한 위치에 바로 접근할 수 있다.

이로 인해 `HashSet`의 `add`와 `contains` 연산은
평균적으로 O(1)에 가까운 시간 복잡도를 가진다.

---

#### 핵심 요약

- HashSet은 내부적으로 `HashMap`을 사용한다  
- 중복 여부는 `hashCode()` → `equals()` 순서로 판단한다  
- 해시 기반 구조로 평균 O(1)의 중복 체크가 가능하다  
- 커스텀 객체는 `hashCode()`와 `equals()`를 함께 재정의해야 한다
